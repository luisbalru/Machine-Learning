\input{preambuloSimple.tex}
\graphicspath{ {./images/} }
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{soul}


%----------------------------------------------------------------------------------------
%	TÍTULO Y DATOS DEL ALUMNO
%----------------------------------------------------------------------------------------

\title{	
\normalfont \normalsize 
\textsc{\textbf{Aprendizaje Automático (2019)} \\ Doble Grado en Ingeniería Informática y Matemáticas \\ Universidad de Granada} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Memoria Práctica 3 \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Luis Balderas Ruiz \\ \texttt{luisbalderas@correo.ugr.es}} 
 % Nombre y apellidos 


\date{\normalsize\today} % Incluye la fecha actual

%----------------------------------------------------------------------------------------
% DOCUMENTO
%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Muestra el Título

\newpage %inserta un salto de página

\tableofcontents % para generar el índice de contenidos

\listoffigures

\listoftables

\newpage


%----------------------------------------------------------------------------------------
%	Introducción
%----------------------------------------------------------------------------------------

\section{Recognition of handwritten digits}

\subsection{Introducción}

Nos enfrentamos a un problema de clasificación multietiqueta (9 clases, con los números del 0 al 9) (aprendizaje supervisado) sobre una base de datos de reconocimiento de dígitos manuscritos, proveniente de la Universidad de Bogazici. Tras extraer mapas de bits de dimensión $32\times32$ normalizados, se dividen en bloques de $4\times4$ disjuntos y se cuenta el número de píxeles en cada bloque. Esto genera una matriz $8\times8$ con entrada en los números enteros del 0 al 16. Más información en \cite{optdigits.names}. 

\subsection{Preprocesado}

El preprocesado de los datos es la parte más importante del pipeline en un proyecto de ciencia de datos. De él se espera refinar, ajustar, completar y, en definitiva, mejorar la congruencia y consistencia de los mismos para conseguir mejores resultados en la parte de análisis y clasificación. Para conseguir un preprocesado más acertado, me baso continuamente en distintas visualizaciones que arrojen pistas sobre los pasos a seguir. Propongo los siguientes apartados:

\subsubsection{Balanceo de las clases} 

Un dataset balanceado es primordial para garantizar un correcto aprendizaje del modelo. En caso de desbalanceo, las clases más representadas tendrían un peso mayor a la hora de etiquetar instancias nuevas en test, de forma que las menos representadas acabarían, con gran probabilidad, mal clasificadas. Cuando se da desbalanceo hay dos posibles alternativas: eliminar instancias de las clases más repetidas (undersampling) o generar nuevas de las clases minoritarias (oversampling). En este último caso, se suelen utilizar algoritmos como SMOTE (\cite{smote}). \\

En nuestro caso, las clases están absolutamente balanceadas:

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|}
		\hline
		Etiqueta & Número de instancias \\ \hline
		0        & 178                  \\ \hline
		1        & 182                  \\ \hline
		2        & 177                  \\ \hline
		3        & 183                  \\ \hline
		4        & 181                  \\ \hline
		5        & 182                  \\ \hline
		6        & 181                  \\ \hline
		7        & 179                  \\ \hline
		8        & 174                  \\ \hline
		9        & 180                  \\ \hline
	\end{tabular}
	\caption{Número de instancias por cada clase}
\end{table}

Veámoslo también gráficamente:

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.6]{count-clases.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Histograma con el número de instancias por clase} 
	\label{fig:clases}
\end{figure}

Por tanto, no es necesario hacer ninguna modificación en ese sentido.

\subsubsection{Variabilidad de los datos}

A continuación, estudiamos la calidad de las características (columnas en la matriz). Para ello, realizo una descripción estadística de los datos. De las 63 características se estudia la variabilidad de cada individuo a través de la media, la desviación típica, el recorrido intercuartílico, máximo, mínimo... Represento la matriz de correlación para estudiar la correlación lineal entre las características:

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.8]{corr-matrix.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Matriz de correlación de características} 
	\label{fig:corr-mat}
\end{figure}

No se aprecia gran correlación lineal entre las variables. Sin embargo, parece que entre la columna 57 y la 1 y la 58 y la 2 (0.822374 y 0.921179 respectivamente) sí hay dependencia lineal.

\subsubsection{Valores perdidos}

Según la documentación del dataset y posteriores análisis exploratorios, no existen valores perdidos en este problema.

\subsubsection{Outliers}

A través de la función definida en el código, basada en el cálculo de $z_{score}$ (\cite{z-score})  para cada instancia, no se ha encontrado ningún outlier. 

\subsubsection{Transformación de variables}

Al ser características discretas (números enteros del 0 al 16) veo muy poco recomendable hacer transformaciones. Las transformaciones de variables están recomendadas especialmente para las características numéricas continuas.

\subsubsection{Regularización}

La literatura indica que la regularización está recomendada en problemas de regresión. No obstante, tras efectuar pruebas con Lasso y Ridge los resultados son muy pobres, por lo que he descartado incluirlo en este documento.

\subsubsection{Normalización}

Como se verá en el siguiente apartado, me dispongo a utilizar Análisis de Componentes Principales (PCA) para reducir el tamaño del dataset y optimizar la información. Paso previo a PCA (y también favorable a la clasificación), realizo una normalización y escalado de los datos (\cite{st-sc}). Para ello, lo que se hace es estandarizar las características con una transformación de localización y escala (quitando la media y dividiendo por la desviación típica):

$$ z = \frac{(x-\mu)}{\sigma} $$ 


En clasificadores basados en distancia, es muy importante normalizar los datos, consiguiendo que estén todas en el mismo rango. Ejemplos de ello son k-NN, SVM con núcleo RBF o el propio modelo lineal.

\subsubsection{Selección de instancias}

Dado que el tamaño del dataset no es muy grande, no veo conveniente reducir el número de individuos, por lo que no hago ninguna selección de instancias y trabajaré de forma permanente con todas las filas del conjunto de datos.

\subsubsection{Selección de características: PCA}

Análisis de componentes principales (PCA, \cite{pca}) es una técnica de estadística multivariante basada en describir un conjunto de datos en términos de otras variables nuevas no correladas. Los componentes se ordenan por la cantidad de varianza original que describen, por lo que es una técnica útil para reducir la dimensionalidad de un conjunto de datos. Como ya he comentado antes, PCA requiere una normalización previa de los datos. El motivo es que el algoritmo calcula una nueva proyección del dataset, en la que los ejes están basados en la desviación típica de las variables. Por tanto, si una variable tiene una gran desviación típica, le será asignada un gran peso en el cálculo de ejes, en detrimento de aquellas características que tengan menor variabilidad. En consecuencia, para un análisis verosímil de los datos es necesario que todas las variables tengan la misma desviación típica. Por otra parte, es evidente que cada variable tiene una unidad de medida y hay que garantizar que en el estudio estadístico todas las características se homogeneizan. 

En nuestro problema, aplico PCA con los parámetros 0.95 y svd\_solver == 'full' (\cite{pca-sk}) para que elija el número mínimo de componentes principales de forma que se garantiza una varianza explicada mayor al 95\%. Tras la ejecución, el número de componentes principales encontrada es 41.

\subsection{Sobre métricas para valorar la clasificación(\cite{seleccion-metricas})}

En un problema de aprendizaje supervisado tenemos múltiples formas de evaluar el rendimiento de nuestros modelos. En el fondo, tratamos de ver qué instancias hemos clasificado bien o mal en función de su etiqueta real. Cuando se tienen problemas de clasificación binaria (clase positiva y clase negativa), surgen los siguientes conceptos a la hora de valorar el rendimiento:

\begin{itemize}
	\item Verdaderos positivos (TP): Instancias correctamente clasificadas como positivas.
	\item Verdaderos negativos (TN): Instancias correctamente clasificadas como negativas.
	\item Falsos positivos (FP): Instancias clasificadas como positivas pero que son negativas.
	\item Falsos negativos (FN): Instancias clasificadas como negativas pero que son positivas.
\end{itemize}

La forma de medir el rendimiento más conocida es el llamado Accuracy, que responde a la siguiente fórmula:

$$\text{Accuracy} = \frac{TP+TN}{TP+FP+FN+TN}$$

Sin embargo, dependiendo de la casuística del problema (sobre todo, si las clases están muy desbalanceadas), es posible que esta medida no nos aporte información veraz. Este hecho genera la necesidad de valorar nuevas métricas:

$$\text{Precision} = \frac{TP}{TP+FP}$$

$$\text{Recall} = \frac{TP}{TP+FN}$$

$$\text{F1-score} = \frac{2*\text{Recall}*\text{Precision}}{\text{Recall}+\text{Precision}}$$

Todas ellas generan una visión más precisa de cómo ha sido nuestra clasificación. Por tanto, serán todas añadidas en la sección siguiente tras la ejecución de cada algoritmo. De igual manera, se expresará la matriz de confusión (\cite{cf}).

\subsection{Clasificación}

Esta práctica está centrada en la utilización de modelos lineales. Por tanto, el primer clasificador que voy a utilizar es SGDClassifier de SKLearn (\cite{SGD-C}), esto  es, un modelo lineal basado en gradiente descendente estocástico. Para garantizar una generalización correcta por parte del modelo, utilizo validación cruzada estratificada con 10 particiones(\cite{stk}). Los resultados de los 10 entrenamientos con las distintas particiones son los siguientes: \\

[0.9584415584415584, 0.948051948051948, 0.9505208333333334, 0.953125, 0.9401041666666666, 0.9348958333333334, 0.9345549738219895, 0.9366754617414248, 0.9472295514511874, 0.9575596816976127] \\


con los siguientes resultados estadísticos:

\begin{itemize}
	\item Media: 0.9461159008539054
	\item Varianza: 8.235657110431097e-05
	\item Mínimo: 0.9345549738219895
	\item Máximo: 0.9584415584415584
\end{itemize}

Por tanto, se puede comprobar que se está generalizando bien por la varianza tan pequeña generada y los resultados adquieren mayor credibilidad. Queda tan solo evaluarla en el conjunto de test: \\

\textbf{Accuracy fuera de la muestra:} 0.9170840289371174.

A continuación, muestro la matriz de confusión:

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.8]{conf-m-sgd.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Matriz de confusión para SGD} 
	\label{fig:conf-m-sgd}
\end{figure}

Para acompañar estas medidas, muestro el resumen completo:

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		& Precision & Recall & F1-Score & Support \\ \hline
		0         & 1.00      & 0.93   & 0.97     & 178     \\ \hline
		1         & 0.88      & 0.94   & 0.91     & 182     \\ \hline
		2         & 0.99      & 0.93   & 0.96     & 177     \\ \hline
		3         & 0.99      & 0.83   & 0.90     & 183     \\ \hline
		4         & 0.94      & 0.97   & 0.95     & 181     \\ \hline
		5         & 0.95      & 0.96   & 0.96     & 182     \\ \hline
		6         & 1.00      & 0.96   & 0.98     & 181     \\ \hline
		7         & 0.99      & 0.86   & 0.92     & 179     \\ \hline
		8         & 0.69      & 0.90   & 0.78     & 174     \\ \hline
		9         & 0.85      & 0.89   & 0.87     & 180     \\ \hline
		avg/total & 0.93      & 0.92   & 0.92     & 1797    \\ \hline
	\end{tabular}
	\caption{Resultados finales de la clasificación con modelo lineal SGD}
\end{table}



A continuación, expreso los resultados del modelo Regresión Logística (\cite{lr}). Realizo de nuevo una validación cruzada con 10 particiones, de forma que obtengo los siguientes resultados: \\

\textbf{Accuracy fuera de la muestra:} 0.943238731219 \\

Matriz de confusión:

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.8]{conf-m-rl.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Matriz de confusión para RL} 
	\label{fig:conf-m-rl}
\end{figure}

Finalmente, muestro un resumen de la clasificación por cada una de las clases:

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		& Precision & Recall & F1-Score & Support \\ \hline
		0         & 0.99      & 0.98   & 0.99     & 178     \\ \hline
		1         & 0.91      & 0.95   & 0.92     & 182     \\ \hline
		2         & 0.97      & 0.96   & 0.97     & 177     \\ \hline
		3         & 0.99      & 0.92   & 0.95     & 183     \\ \hline
		4         & 0.95      & 0.97   & 0.96     & 181     \\ \hline
		5         & 0.90      & 0.99   & 0.94     & 182     \\ \hline
		6         & 0.98      & 0.98   & 0.98     & 181     \\ \hline
		7         & 0.99      & 0.90   & 0.94     & 179     \\ \hline
		8         & 0.91      & 0.89   & 0.90     & 174     \\ \hline
		9         & 0.86      & 0.91   & 0.88     & 180     \\ \hline
		avg/total & 0.95      & 0.94   & 0.94     & 1797    \\ \hline
	\end{tabular}
	\caption{Resultados finales de la clasificación con modelo lineal Regresión Lineal}
\end{table}

\newpage

Por último, evalúo el algoritmo SVM (\cite{svc}). Lo primero que hay que hacer es elegir los parámetros con los que se ejecutará el algoritmo. Es necesario elegir $C$ (parámetro de penalización para el término del error), $kernel$ y $gamma$ (coeficiente para el núcleo). Para elegir el mejor conjunto de coeficientes, llevo a cabo un GridSearch con validación cruzada de 10 particiones, asegurando así que elijo los mejores parámetros independientemente de la partición que se haga en los datos. Dicho proceso es costoso computacionalmente y requiere un tiempo (alrededor de unos minutos, dependiendo del ordenador donde se ejecute). Los mejores resultados son los siguientes:

$$ C = 1000000$$
$$ \gamma = 1^{-5} $$
$$ kernel = rbf $$

Con dichos parámetros, se obtienen los siguientes resultados: \\

\textbf{Accuracy fuera de la muestra:}  0.9577072899276572\\

Matriz de confusión:

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.8]{conf-m-svm.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Matriz de confusión para SVM} 
	\label{fig:conf-m-svm}
\end{figure}

\newpage
Resumen de la clasificación:


\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		& Precision & Recall & F1-Score & Support \\ \hline
		0         & 0.99      & 1.00   & 0.99     & 178     \\ \hline
		1         & 0.91      & 0.99   & 0.95     & 182     \\ \hline
		2         & 0.98      & 0.96   & 0.97     & 177     \\ \hline
		3         & 0.96      & 0.95   & 0.96     & 183     \\ \hline
		4         & 0.95      & 0.99   & 0.97     & 181     \\ \hline
		5         & 0.93      & 0.99   & 0.96     & 182     \\ \hline
		6         & 1.00      & 0.98   & 0.99     & 181     \\ \hline
		7         & 1.00      & 0.91   & 0.95     & 179     \\ \hline
		8         & 0.96      & 0.90   & 0.93     & 174     \\ \hline
		9         & 0.92      & 0.89   & 0.91     & 180     \\ \hline
		avg/total & 0.96      & 0.96   & 0.96     & 1797    \\ \hline
	\end{tabular}
	\caption{Resultados finales de la clasificación con SVM}
\end{table}

\subsection{Conclusiones}

A la vista de los resultados, podemos asegurar que el conjunto de datos se presta bien a ser separado de forma lineal, ya que las cotas de error (no mayores al 5\% de accuracy, pero también precision, recall o medida F1) son muy leves. Si estudiamos de forma más detenida las matrices de confusión, podemos ver que en SGD el número 3 es el que peores resultados arroja. La confusión viene con el número 8. Parece lógico pensar que, dadas las características del trazo de ambos números, se puedan confundir. Regresión logística mejora el rendimiento para el 3 y obtiene su peor resultado en el 8. Por último, SVM obtiene su peor resultado para el 9. A nivel general, podemos decir que la normalización y la selección de características vía PCA han conseguido generar unos datos susceptibles de ser separados por modelos lineales con grandes resultados.

\newpage 
\section{Airfoil self noise}

\subsection{Introducción}

Nos encontramos ante un problema de regresión (aprendizaje supervisado) proveniente de la NASA y alojado en \cite{airfoil-data}. En él se reflejan los datos de una serie de pruebas acústicas y aerodinámicas sobre secciones de aspas en 2 y 3 dimensiones tras ser evaluadas en un túnel de viento. El conjunto de datos consta de 1503 instancias y 6 atributos, que son los siguientes:

\begin{itemize}
	\item Frecuencia (Hertzs)
	\item Ángulo de ataque (grados)
	\item Longitud de la cuerda (metros)
	\item Velocidad de la corriente (m/s)
	\item Espesor de desplazamiento lateral en la succión (en metros)
	\item Nivel de presión de sonido escalado (decibelios) 
\end{itemize}

siendo esta última la salida. En el presente documento se pretende abordar un procesado y análisis de los datos para encontrar una configuración y un regresor óptimo. Comienzo visualizando la distribución de los datos y la correlación lineal de las variables. Los resultados de dicho análisis nos darán idea sobre qué variables son más o menos trascendentes en el estudio. Además, será necesario aplicar normalización sobre los datos, tratamiento de outliers o valores perdidos.

\newpage

\subsection{Preprocesado}

En primer lugar, muestro la matriz de correlación de las variables junto con el coeficiente correspondiente.
\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.6]{corr-airfoil.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Matriz de correlación entre las variables} 
	\label{fig:corr-airfoil}
\end{figure}

Como se puede comprobar, la correlación lineal entre las variables es baja. Eso nos indica que el dataset no es redundante y que todas las variables nos aportarán información por sí solas. Tan sólo podemos encontrar dos variables que sí tienen una alta correlación entre ellas: 'Suction-thickness' y 'Angle-Attack', con 0.75 de coeficiente de correlación lineal. Por tanto, podríamos plantearnos eliminar alguna de ellas porque son mutuamente explicables. Además, 'Suction-thickness' Y 'Chord-length' mantienen una correlación inversa de 0.5. Para obtener más información, visualizo la distribución de las variables y su relación a través de un pairplot.

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.6]{pairplot-air.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Pairplot para el dataset Airfoil} 
	\label{fig:pairplot-air}
\end{figure}

Podemos ver ahora con más claridad la correlación entre las variables antes comentadas. Por tanto, podremos considerar la eliminación de alguna de esas características.


\subsubsection{Valores perdidos}

La documentación del conjunto de datos y las exploraciones posteriores indican que no hay valores perdidos.
\newpage
\subsubsection{Outliers}

Seguidamente, muestro un resumen estadístico de las variables:

\begin{table}[H]
	\resizebox{16cm}{!}{
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		& Frequency & Angle-Attack & Chord-Length & Free-Stream Velocity & Suction-tickness & SSPresure-level \\ \hline
		count & 1.00      & 1503         & 1503         & 1503                 & 1503             & 1503            \\ \hline
		mean  & 0.92      & 2886.380572  & 0.136548     & 50.860745            & 0.01114          & 124.835943      \\ \hline
		std   & 0.99      & 3152.573137  & 0.093541     & 15.572784            & 0.01315          & 6.898657        \\ \hline
		min   & 0.99      & 200          & 0.0254       & 31.7                 & 0.000401         & 103.38          \\ \hline
		25\%  & 0.95      & 800          & 0.0508       & 39.6                 & 0.002535         & 120.191         \\ \hline
		50\%  & 0.93      & 1600         & 0.1016       & 39.6                 & 0.004957         & 125.721         \\ \hline
		75\%  & 1.00      & 4000         & 0.2286       & 71.3                 & 0.015576         & 129.9955        \\ \hline
		max   & 0.98      & 20000        & 22.2         & 71.3                 & 0.058411         & 140.987         \\ \hline
	\end{tabular}}
	\caption{Resumen estadístico de las variables}
\end{table}

Como vemos, la variable 'Frequency' es la que más desviación típica tiene, por lo que podría generar más outliers. Me centro en esa característica y la comparo con nuestro target, para ver si hay datos que se desvían mucho:

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.6]{freq-ssp.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Relación entre la variable Frequency y SSPresure-Level} 
	\label{fig:freq-ssp}
\end{figure}

Los datos de 'SSPresure-Level' menores que 105Db y mayores que 142Db podrían ser  outliers. Por tanto, los elimino. Veamos el resultado:

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.6]{sin-out-air.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Resultado tras eliminar outliers} 
	\label{fig:sin-out}
\end{figure}

\subsubsection{Transformaciones en las variables}

La asimetría va a ser la propiedad que defina si una variable necesita ser transformada o no. En el caso de asimetría positiva, aplico una tranformación logarítmica y, en el caso de una asimetría negativa, elevo al cuadrado (\cite{skew1},\cite{skew2}). Según la literaratura, un coeficiente de asimetría de entre -0.5 y 0.5 se denomina asimetría leve. Para valores superiores a 1 (inferiores a -1), la asimetría es alta. Muestro, por tanto, las gráficas y coeficientes de asimetría de cada variable:

\begin{itemize}
	\item 'Frequency'

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.412]{dist-fr.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Distribución de Frequency} 
	\label{fig:dist-freq}
\end{figure}

Coeficiente de asimetría: 2.141010620719073

\item 'Angle-Attack'

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.45]{dist-aa.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Distribución de Angle Attack} 
	\label{fig:dist-aa}
\end{figure}

Coeficiente de asimetría 0.6954226765390694

\item 'Chord-Length'

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.45]{dist-cl.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Distribución de Chord-Length} 
	\label{fig:dist-cl}
\end{figure}

Coeficiente de asimetría: 0.4552190466982675

\item 'Free Stream Velocity'

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.45]{dist-fsv.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Distribución de Free stream velocity} 
	\label{fig:dist-fsv}
\end{figure}

Coeficiente de asimetría: 0.2339049705934876

\item 'Suction-thickness'

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.45]{dist-st.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Distribución de Suction thickness} 
	\label{fig:dist-st}
\end{figure}

Coeficiente de asimetría: 1.6997773651215724

\item SSPresure-Level

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.46]{dist-sspr.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Distribución de SSPresure-Level} 
	\label{fig:dist-sspr}
\end{figure}

Coeficiente de asimetría: -0.39348332062796404

Estudio también la gráfica de probabilidad:

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.46]{prob-ssp.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Probabilidad de SSPresure-Level} 
	\label{fig:prob-sspr}
\end{figure}
\end{itemize}

Por tanto, todas las variables sufren una asimetría positiva excepto SSPresure-Level, cuya asimetría es negativa. Aplico las transformaciones correspondientes. Como Angle-Attack contiene 0, no es conveniente aplicar la transformación logarítmica. La literatura recomienda que se utilice otra, como por ejemplo la raíz cuadrada.

\subsubsection{Normalización}

Tras la transformación de las variables, aplico una normalización basada en z-score (con la media y la desviación típica, como en el ejercicio de clasificación) vía la librería Standard-Scaler (\cite{st-sc}). Es trascendental aplicarla para así mejorar los resultados de la regresión.


\subsubsection{Reducción de datos}

El tamaño del conjunto de datos es reducido (tan solo 5 características) por lo que no tiene sentido hacer reducción de las mismas. Además, tras el estudio de su correlación, vemos que son más o menos independientes (a excepción de 2) por lo que, en principio, es posible que nos den información valiosa. Además, necesitaremos a continuación dividir el dataset en entrenamiento y test, por lo que si reducimos instancias, vamos a perder capacidad de generalización.

\subsection{Regresión}

Una vez entendidos, analizados y preprocesados los datos, llega el momento de aplicar los algoritmos de regresión y evaluar los resultados. Antes de comenzar con los algoritmos, me dispongo a comentar cómo se mide la bondad del ajuste en regresión.

\subsubsection{Medida de la bondad: Coeficiente de determinación}

El coeficiente de determinación $R^2$ \cite{rcuadrado} (cuadrado del coeficiente de correlación de Pearson) es el estadístico con el que medimos el ajuste de la regresión (en este caso lineal). Toma valores entre 0 y 1, siendo 0 una total incorrelación entre la función solución generada y la real que queremos ajustar y 1, una total correlación. En un plano general, si llamamos $\sigma^2$ a la varianza de la variable dependiente y $\sigma^2_r$ la varianza residual \cite{var-res}, el coeficiente de determinación se calcula de acuerdo a la siguiente fórmula:

$$ \rho^2 = 1 - \frac{\sigma^2_r}{\sigma^2}$$

Si la regresión es lineal, entonces basta con hacer el cuadrado al coeficiente de correlación de Pearson:

$$R^2 = \frac{\sigma^2_{XY}}{\sigma^2_X \sigma^2_Y}$$

donde $\sigma_{XY}$ es la covarianza de (X,Y), $\sigma^2_X$ es la varianza de la variable X y $\sigma^2_Y$ es la varianza de la variable Y.

\subsubsection{Medida del error: Error absoluto medio}

Para medir el error cometido en el modelo, elijo la métrica Error Absoluto Medio (MAE) debido a su fácil interpretabilidad (se define como la medida de la diferencia entre dos variables continuas. En nuestro caso, esas dos variable son la predicción que hacemos y los valores reales del target). El error absoluto medio viene dado por la siguiente expresión:

$$MAE = \frac{\sum_{i=1}^{n}|y_i-x_i|}{n}$$

Toma valores entre 0 y $\infty$, siendo 0 el mejor valor.

\subsubsection{Regresión lineal}

Utilizo en primer lugar el algoritmo básico de regresión lineal implementado en SKLearn (\cite{reg-l}). Tras hacer el ajuste en el conjunto de entrenamiento, evalúo el test obteniendo unos resultados muy pobres fuera de la muestra:

$$R^2 = 0.4003157970532767$$
$$MAE = 0.5970386315572326$$
\subsubsection{ElasticNet}

ElasticNet (\cite{elastic}) es un regresor lineal que combina la regularización de Lasso y de Ridge. En este caso, utilizo validación cruzada de 10 particiones para elegir el mejor modelo y evitar el sobreajuste. Sin embargo, los resultados siguen siendo muy pobres:

$$R^2 = 0.40021812841638726$$
$$MAE = 0.5972005818175051$$

Todo parece indicar que los regresores lineales no están sabiendo generalizar el conjunto de datos. Además, producen los mismos errores

\subsubsection{Perceptron multicapa (redes neuronales)}

En un intento de reconducir los malos resultados recibidos por parte de los regresores lineales, utilizo el perceptron multicapa (\cite{mlp}) como aproximador universal para encontrar mejor rendimiento. Los parámetros utilizados son los que vienen por defecto (impresos en la ejecución). Una alternativa sería hacer un gridsearch para buscar los más convenientes. El coeficiente de determinación devuelto por la red neuronal es mucho más halagüeño:

$$R^2 = 0.9091615231543956$$
$$MAE = 0.22627734611443473$$

EL coeficiente de determinación es muy alto y el error se reduce a la mitad que en los modelos lineales.

Como se ve en la ejecución, no encontramos problemas de sobreaprendizaje en ningún caso ya que las medidas en training y test son muy parecidas.

\subsection{SVR}

Por último, utilizo la modalidad de SVM para regresión, SVR (\cite{svr}) con parámetros $C=1, \epsilon = 0.2$. De nuevo, un gridsearch nos ayudaría en la elección de los mejores hiperparámetros. Los resultados también son positivos:

$$R^2 = 0.8535264640850369$$
$$MAE = 0.27968782470229836$$


\subsection{Mejora: Modificación del dataset}
Como se puede ver, los regresores lineales han tenido muy malos resultados. Los otros dos algoritmos mejoran el panorama, pero no podemos soslayar el motivo de dichos malos resultados. En ciencia de datos nos ocupamos muchas veces del sobreajuste y en este caso, con todo el preprocesamiento y la validación cruzada (cuando corresponde) hemos evitado que ocurra. Sin embargo, nuestro problema aquí es el llamado \textit{under-fitting}, es decir, necesitamos aumentar la complejidad del modelo ya que tenemos un gran sesgo. Para ello, me dispongo a crear nuevas características al dataset que son combinaciones lineales de las variables elevándolas hasta grado 3. Utilizo la librería \textit{PolynomialFeatures} (\cite{pf}), en la que se genera una nueva matriz de características formada por todas las combinaciones de tipo polinómico de hasta el grado indicado. He elegido 3 en un proceso de ensayo-error, intentando introducir la menor complejidad posible y subiendo los resultados. Tras esa modificación sobre los datos, vuelvo a ejecutar los algoritmos, obteniendo los siguientes resultados:

\begin{center}

\begin{itemize}
	\centering
	\item[\textbf{Regresor Lineal:}] $R^2 = 0.8020312507944377, MAE = 0.33250852908432094$
	\item[\textbf{ElasticNet:}] $R^2 = 0.8027327562232623, MAE = 0.332467604747769$
	\item[\textbf{Red Neuronal:}] $R^2 = 0.9505089732627274, MAE = 0.15475588581572994$
	\item[\textbf{SVR:}] $R^2 = 0.8844166670604567, MAE = 0.2449328853236184$
\end{itemize}
\end{center}

Como se puede apreciar, en todos los casos mejora, por lo que el nuevo dataset sí que se presta a la regresión lineal, encontrándose una reducción del error muy sustancial y un crecimiento de $R^2$ notable.
\newpage
\section{Bibliografía}

%------------------------------------------------

\bibliography{citas} %archivo citas.bib que contiene las entradas 
\bibliographystyle{plain} % hay varias formas de citar

\end{document}
